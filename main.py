import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig
from PIL import Image
import time
import os

# ==========================================
# CONFIGURATION
# ==========================================
MODEL_ID = "scb10x/typhoon-ocr-7b"
IMAGE_PATH = "test.jpeg" 

def load_model_and_processor():
    """‡πÅ‡∏¢‡∏Å‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î"""
    print(f"--- üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î Model (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏ó‡∏≥‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß) ---")
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True
    )

    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)
    
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        attn_implementation="sdpa" # ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Windows
    )
    
    # ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á Model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Inference
    model.eval() 
    return model, processor

def run_typhoon_ocr(model, processor, image_path):
    if not os.path.exists(image_path):
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û: {image_path}")
        return

    # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÅ‡∏ö‡∏ö‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß
    image = Image.open(image_path).convert("RGB")
    
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": "Extract all text from this image accurately."},
            ],
        }
    ]

    # --- ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏£‡πà‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß (Visual Token Optimization) ---
    # ‡∏ö‡∏µ‡∏ö‡∏û‡∏¥‡∏Å‡πÄ‡∏ã‡∏•‡∏•‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Process ‡πÑ‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏î‡∏Ç‡∏≠‡∏á OCR
    # 600,000 ‡∏û‡∏¥‡∏Å‡πÄ‡∏ã‡∏• ‡∏Ñ‡∏∑‡∏≠‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Speed ‡πÅ‡∏•‡∏∞ Accuracy
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = processor(
        text=[text],
        images=[image],
        padding=True,
        return_tensors="pt",
        min_pixels=256 * 28 * 28,
        max_pixels=800 * 28 * 28  # ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô 800 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏Ñ‡∏á‡∏ó‡∏µ‡πà
    ).to("cuda")

    print(f"--- üîç ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ (Tokens: {inputs.input_ids.shape[1]}) ---")
    
    start_time = time.time()
    
    # 2. ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (‡∏à‡∏π‡∏ô‡∏Ñ‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Speed)
    with torch.inference_mode():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=1024,
            do_sample=False,
            use_cache=True,             # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏ó‡∏ß‡∏µ‡∏Ñ‡∏π‡∏ì
            pad_token_id=processor.tokenizer.pad_token_id,
            eos_token_id=processor.tokenizer.eos_token_id,
        )

    # 3. ‡∏ñ‡∏≠‡∏î‡∏£‡∏´‡∏±‡∏™‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    generated_ids = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, output_ids)
    ]
    result = processor.batch_decode(
        generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )[0]

    elapsed_time = time.time() - start_time
    
    print("\n" + "="*40)
    print(f"‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {elapsed_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
    print("-" * 40)
    print(f"‚ú® ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:\n{result}")
    print("="*40)
    
    # ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå Cache ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏∑‡∏ô RAM ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á)
    torch.cuda.empty_cache()

if __name__ == "__main__":
    if torch.cuda.is_available():
        # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô
        model, processor = load_model_and_processor()
        
        # ‡∏£‡∏±‡∏ô OCR
        run_typhoon_ocr(model, processor, IMAGE_PATH)
    else:
        print("‚ùå Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö GPU")