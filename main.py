# 20-35 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‡πÅ‡∏ï‡πà‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å

import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig
from PIL import Image
import time
import os

MODEL_ID = "scb10x/typhoon-ocr-7b"
IMAGE_PATH = "test.jpeg"

def load_model_and_processor():
    print(f"--- üöÄ Load Model (Optimized for 8GB VRAM) ---")
    
    bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16, # ‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô float16 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î overhead ‡∏ö‡∏ô‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å
    bnb_4bit_use_double_quant=True # ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î VRAM ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏≠‡∏µ‡∏Å‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 200-400MB
    )
    
    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)
    
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_config,
    device_map="auto", # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å "cuda" ‡πÄ‡∏õ‡πá‡∏ô "auto" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ memory ‡πÑ‡∏î‡πâ‡∏â‡∏•‡∏≤‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
    trust_remote_code=True,
    torch_dtype=torch.float16, # ‡πÉ‡∏ä‡πâ float16 ‡πÉ‡∏´‡πâ‡πÅ‡∏°‡∏ï‡∏ä‡πå‡∏Å‡∏±‡∏ö bnb_config
    attn_implementation="sdpa"
    )
    model.eval()
    return model, processor

def run_typhoon_ocr(model, processor, image_path):
    if not os.path.exists(image_path): return
    
    # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
    raw_image = Image.open(image_path).convert("RGB")
    
    # üéØ ‡∏à‡∏∏‡∏î‡πÅ‡∏Å‡πâ 1: ‡∏•‡∏î Pixel Limit
    # 336 * 28 * 28 ‡∏Ñ‡∏∑‡∏≠‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (Sweet Spot) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• 7B 
    # ‡∏•‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Token ‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡∏•‡∏á‡πÑ‡∏î‡πâ‡∏°‡∏´‡∏≤‡∏®‡∏≤‡∏• ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ä‡∏±‡∏î
    pixel_limit = 768 * 28 * 28 

    messages = [{"role": "user", "content": [
        {"type": "image", "image": raw_image},
        {"type": "text", "text": "Extract all Thai text accurately."} 
    ]}]
    
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Inputs
    inputs = processor(
        text=[text],
        images=[raw_image],
        return_tensors="pt",
        min_pixels=256 * 28 * 28,
        max_pixels=pixel_limit
    ).to("cuda")

    print(f"--- ‚öôÔ∏è Processing Image... ---")
    start_time = time.time()
    
    # üéØ ‡∏à‡∏∏‡∏î‡πÅ‡∏Å‡πâ 2: Generate Settings
    with torch.inference_mode():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=2412, # üéØ ‡∏•‡∏î‡∏à‡∏≤‡∏Å 1024 ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏¢‡∏≤‡∏ß‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏≤‡∏©
            do_sample=False,    # ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ß‡πÅ‡∏•‡∏∞‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
            use_cache=True,     # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ß‡πâ‡πÄ‡∏™‡∏°‡∏≠
            num_beams=1,
            pad_token_id=processor.tokenizer.pad_token_id,
            eos_token_id=processor.tokenizer.eos_token_id,
        )

    generated_ids = output_ids[:, inputs.input_ids.shape[1]:]
    result = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

    elapsed = time.time() - start_time
    print("-" * 30)
    print(f"‚ú® ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:\n{result}")
    print(f"\n‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {elapsed:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")

if __name__ == "__main__":
    # üéØ ‡∏à‡∏∏‡∏î‡πÅ‡∏Å‡πâ 3: ‡∏•‡∏ö torch.compile ‡∏≠‡∏≠‡∏Å‡∏Å‡πà‡∏≠‡∏ô
    # ‡∏ö‡∏ô Windows torch.compile ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡∏ä‡πâ‡∏≤‡πÑ‡∏õ 5-10 ‡∏ô‡∏≤‡∏ó‡∏µ (‡∏™‡∏∞‡∏™‡∏°‡πÉ‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡πá‡∏ô)
    model, processor = load_model_and_processor()
    
    run_typhoon_ocr(model, processor, IMAGE_PATH)