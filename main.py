import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig
from PIL import Image
import time
import os

MODEL_ID = "scb10x/typhoon-ocr-7b"
IMAGE_PATH = "test.jpeg"

def load_model_and_processor():
    print(f"--- üöÄ Loading Model with Optimization ---")
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True
    )

    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)
    
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        MODEL_ID,
        quantization_config=bnb_config,
        device_map="cuda", # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏•‡∏á GPU ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡∏ñ‡πâ‡∏≤ error ‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤ VRAM ‡πÑ‡∏°‡πà‡∏û‡∏≠)
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        # ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ RTX 3000/4000 ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô "flash_attention_2"
        attn_implementation="sdpa" 
    )
    
    model.eval()
    return model, processor

def run_typhoon_ocr(model, processor, image_path):
    if not os.path.exists(image_path): return

    image = Image.open(image_path).convert("RGB")
    
    # 1. ‡∏•‡∏î‡∏û‡∏¥‡∏Å‡πÄ‡∏ã‡∏•‡∏•‡∏á (448*28*28 ‚âà 156,800 pixels) 
    # ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏à‡∏∏‡∏î‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß!
    min_pixels = 256 * 28 * 28
    max_pixels = 448 * 28 * 28 

    messages = [{"role": "user", "content": [{"type": "image", "image": image}, {"type": "text", "text": "Extract text"}]}]
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    inputs = processor(
        text=[text],
        images=[image],
        return_tensors="pt",
        min_pixels=min_pixels,
        max_pixels=max_pixels
    ).to("cuda")

    start_time = time.time()
    
    # 2. ‡∏õ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Generate ‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡∏Ç‡∏∂‡πâ‡∏ô
    with torch.inference_mode():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=512, # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 1024 ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏¢‡∏≠‡∏∞‡∏°‡∏≤‡∏Å
            do_sample=False,
            use_cache=True,
        )

    # 3. Decode ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
    generated_ids = output_ids[:, inputs.input_ids.shape[1]:]
    result = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

    print(f"‚è±Ô∏è Time taken: {time.time() - start_time:.2f} seconds")
    print(f"‚ú® Result: {result}")

if __name__ == "__main__":
    if torch.cuda.is_available():
        model, processor = load_model_and_processor()
        run_typhoon_ocr(model, processor, IMAGE_PATH)